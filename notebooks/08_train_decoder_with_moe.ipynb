{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weird-gamma",
   "metadata": {},
   "source": [
    "# 08: Train Decoder-Only Model with Mixture of Experts (MoE)\n",
    "In this notebook, we demonstrate how to train a GPT-style decoder model using Mixture of Experts (MoE) layers.\n",
    "\n",
    "**Key features:**\n",
    "- Multi-domain synthetic dataset (poetry, news, code, dialog)\n",
    "- MoE used in place of standard FFN blocks\n",
    "- Gating mechanism learns to route token representations to different experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-residence",
   "metadata": {},
   "source": [
    "## Create synthetic multi-domain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "domains = {\n",
    "    \"poetry\": \"Shall I compare thee to a summer's day?\",\n",
    "    \"news\": \"The central bank announced an increase in interest rates.\",\n",
    "    \"code\": \"def add(a, b): return a + b\",\n",
    "    \"dialog\": \"Hey! How are you doing today?\"\n",
    "}\n",
    "\n",
    "samples = []\n",
    "for domain, prompt in domains.items():\n",
    "    for _ in range(300):\n",
    "        samples.append(f\"<{domain}> {prompt}\")\n",
    "random.shuffle(samples)\n",
    "text = \"\\n\".join(samples)\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-alaska",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, block_size):\n",
    "        self.examples = [\n",
    "            torch.tensor(tokens[i:i+block_size+1])\n",
    "            for i in range(len(tokens) - block_size - 1)\n",
    "        ]\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.examples[idx]\n",
    "        return data[:-1], data[1:]\n",
    "\n",
    "block_size = 64\n",
    "dataset = TextDataset(tokens, block_size)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-crown",
   "metadata": {},
   "source": [
    "## MoE Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, k=2):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.w_gating = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_logits = self.w_gating(x)  # [B, T, E]\n",
    "        topk_vals, topk_idx = torch.topk(gate_logits, self.k, dim=-1)\n",
    "        topk_weights = F.softmax(topk_vals, dim=-1)  # [B, T, K]\n",
    "        return topk_weights, topk_idx\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts, k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, input_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gate = TopKGate(input_dim, num_experts, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz, seq_len, dim = x.shape\n",
    "        weights, indices = self.gate(x)  # [B, T, K], [B, T, K]\n",
    "        x_flat = x.view(-1, dim)\n",
    "        output = torch.zeros_like(x)\n",
    "\n",
    "        for i in range(self.k):\n",
    "            idx = indices[:, :, i]  # [B, T]\n",
    "            mask = F.one_hot(idx, num_classes=self.num_experts).float()  # [B, T, E]\n",
    "            mask = mask.permute(2, 0, 1)  # [E, B, T]\n",
    "            for e in range(self.num_experts):\n",
    "                selected = mask[e] > 0  # [B, T]\n",
    "                if selected.any():\n",
    "                    selected_flat = selected.view(-1)\n",
    "                    input_sel = x_flat[selected_flat]\n",
    "                    output_sel = self.experts[e](input_sel)\n",
    "                    scaled_output = weights[:, :, i].reshape(-1)[selected_flat].unsqueeze(1) * output_sel\n",
    "                    output.view(-1, dim)[selected_flat] += scaled_output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-escape",
   "metadata": {},
   "source": [
    "## Define the Decoder Model with MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoETransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_experts=4, k=2):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ff = MoELayer(embed_dim, hidden_dim=embed_dim * 4, num_experts=num_experts, k=k)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        x = self.ln2(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class MoEGPTDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, depth, heads, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MoETransformerBlock(embed_dim, heads) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embed(x) + self.pos_embed[:, :x.size(1)]\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.lm_head(self.norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-advantage",
   "metadata": {},
   "source": [
    "## Train the MoE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MoEGPTDecoder(\n",
    "    vocab_size=len(tokenizer),\n",
    "    embed_dim=512,\n",
    "    depth=4,\n",
    "    heads=8,\n",
    "    max_len=block_size\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"moe_decoder_trained.pt\")\n",
    "print(\"âœ… MoE model trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-portfolio",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
